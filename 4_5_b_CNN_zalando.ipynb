{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# Mejora de la precisión de la visión artificial mediante convoluciones\n",
        "\n",
        "En la práctica 4.3, vimos cómo hacer el reconocimiento de artículos de moda usando una red neuronal profunda (DNN) que contiene tres capas: la capa de entrada (en la forma de los datos), la capa de salida (en la forma de la salida deseada) y una capa oculta. Experimentaste con el impacto de diferentes tamaños de capa oculta, número de épocas de entrenamiento, etc. en la precisión final.\n",
        "\n",
        "Para mayor comodidad, aquí está el código completo de nuevo. Ejecútalo y toma nota de la precisión de la prueba que se imprime al final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcsRtq9OLorS",
        "outputId": "fb9debe2-7a39-4e9d-92f2-38c3890c220e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: 0.4940 - accuracy: 0.8269\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3738 - accuracy: 0.8654\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3337 - accuracy: 0.8776\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3118 - accuracy: 0.8854\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2952 - accuracy: 0.8914\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3553 - accuracy: 0.8738\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "La precisión es probablemente del 89 % en el entrenamiento y del 87 % en la validación... no está mal... ¿Pero cómo puede mejorarse aún más? Una forma es usar Convoluciones: reducen el contenido de la imagen para centrarse en detalles específicos y distintos.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
        "\n",
        "En resumen, una convolución toma una matriz (generalmente 3x3 o 5x5) y la pasa sobre la imagen. Al cambiar los píxeles subyacentes según la fórmula dentro de esa matriz, se puede hacer cosas como la detección de bordes. Un ejemplo típico para detectar bordes es una matriz de 3x3 donde la celda del medio es 8 y todos sus vecinos son -1. \n",
        "En este caso, cada píxel multiplicaría su valor por 8 y luego restaría el valor de cada vecino. Y esto para cada píxel obteniendo una nueva imagen con los bordes mejorados.\n",
        "\n",
        "Esto es esencial en visión artificial, porque a menudo son las características que se pueden resaltar de esta manera las que distinguen un elemento de otro, y la cantidad de información necesaria es mucho menor... porque solo entrenará en las características resaltadas.\n",
        "\n",
        "Ese es el concepto de redes neuronales convolucionales. Agregue algunas capas para hacer la convolución antes de tener las capas densas, y luego la información que va a las capas densas es más enfocada y posiblemente más precisa.\n",
        "\n",
        "Ejecuta el siguiente código: esta es la misma red neuronal que antes, pero esta vez con capas convolucionales agregadas primero. Tomará más tiempo, pero mira el impacto en la precisión:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0tFgT1MMKi6",
        "outputId": "9ae39dc4-09e7-4f20-d3db-6879a99496fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 95s 50ms/step - loss: 0.4264 - accuracy: 0.8448\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 91s 49ms/step - loss: 0.2914 - accuracy: 0.8932\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 93s 49ms/step - loss: 0.2480 - accuracy: 0.9086\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 91s 48ms/step - loss: 0.2157 - accuracy: 0.9194\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.1874 - accuracy: 0.9291\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.2715 - accuracy: 0.9042\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 épocas\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn1YzZ8D_cPm",
        "outputId": "7d6ca2bd-b206-4911-a7c7-00614a5f1e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.4432 - accuracy: 0.8380\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.2955 - accuracy: 0.8918\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.2503 - accuracy: 0.9072\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.2177 - accuracy: 0.9191\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.1933 - accuracy: 0.9270\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.1687 - accuracy: 0.9359\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.1500 - accuracy: 0.9439\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.1317 - accuracy: 0.9505\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.1149 - accuracy: 0.9568\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.1021 - accuracy: 0.9615\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0871 - accuracy: 0.9671\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.0789 - accuracy: 0.9700\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0675 - accuracy: 0.9738\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.0626 - accuracy: 0.9763\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0584 - accuracy: 0.9780\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0496 - accuracy: 0.9815\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.0474 - accuracy: 0.9825\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0446 - accuracy: 0.9831\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.0410 - accuracy: 0.9849\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 89s 48ms/step - loss: 0.0342 - accuracy: 0.9873\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.5356 - accuracy: 0.9099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        "Es probable que haya subido a alrededor del 93 % en los datos de entrenamiento y al 91 % en los datos de validación.\n",
        "\n",
        "¡esto es muy significativo y es un paso en la dirección correcta!\n",
        "\n",
        "Intenta ejecutarlo durante más épocas, digamos 20 épocas y explora los resultados. Pero si bien los resultados pueden parecer realmente buenos, los resultados de la validación en realidad pueden disminuir, debido al \"sobreajuste\":\n",
        "\n",
        "El 'sobreajuste' ocurre cuando la red aprende muy bien los datos del conjunto de entrenamiento, pero está demasiado especializada para solo esos datos y, como resultado, es menos eficaz para ver *otros* datos. Por ejemplo, si toda tu vida solo viste zapatos rojos, entonces cuando ves un zapato rojo serías muy bueno para identificarlo, pero los zapatos de tacón pueden confundirte... Es algo así como \"aprenderse de memoria\" el conjunto de datos, sin haber entendido cómo generalizar.\n",
        "\n",
        "Mira el código nuevamente y observa, paso a paso, cómo se construyen las convoluciones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLX5cgI_JDb"
      },
      "source": [
        "El paso 1 es recopilar los datos. Notarás que hay un pequeño cambio aquí, ya que los datos de entrenamiento necesitan ser remodelados. Esto se debe a que la primera convolución espera un solo tensor que contenga todo, por lo que en lugar de 60000 elementos de 28x28x1 en una lista, tenemos una sola lista 4D de 60000x28x28x1, y lo mismo para las imágenes de prueba. Si no haces esto, te dará un error al entrenar, ya que las convoluciones no reconocen la forma.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_W_INc_kJQ"
      },
      "source": [
        "Lo siguiente es definir el modelo. Ahora, en lugar de la capa de entrada en la parte superior, se agregará una Convolución. Los parámetros son:\n",
        "\n",
        "1. El número de convoluciones que desea generar. Puramente arbitrario, pero bueno para empezar con algo del orden de 32\n",
        "2. El tamaño de la Convolución, en este caso una cuadrícula de 3x3\n",
        "3. La función de activación que se usará; en este caso, usaremos relu (equivalente a devolver x cuando x>0, y en caso contrario, devolver 0)\n",
        "4. En la primera capa, la forma de los datos de entrada.\n",
        "\n",
        "Después de la convolución se añade una capa MaxPooling que está diseñada para comprimir la imagen, mientras mantiene el contenido de las características que fueron resaltadas por la convolución. Al especificar (2,2) para MaxPooling, el efecto es reducir a la cuarta parte el tamaño de la imagen. Sin entrar en demasiados detalles aquí, la idea es que crea una matriz de píxeles de 2x2 y elegir el más grande, convirtiendo así 4 píxeles en 1. Repite esto en toda la imagen y, al hacerlo, reduce a la mitad el número de píxeles horizontales, y reduce a la mitad el número de píxeles verticales, reduciendo efectivamente la imagen en un 25%.\n",
        "\n",
        "Se invoca a model.summary() para ver el tamaño y la forma de la red, y notarás que después de cada capa de MaxPooling, el tamaño de la imagen se reduce.\n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA"
      },
      "source": [
        "añadir otra convolución más\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Y hacer un Flatten a la salida. Después de todo este proceso, tendrás la misma estructura que la DNN de la práctica 4.3\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GSsjUhAaSj"
      },
      "source": [
        "Compilamos el modelo, llamamos al método fit y evaluamos la pérdida y la precisión con el conjunto de test:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd4P0Kw99VPU",
        "outputId": "42d10b44-1be8-42fd-f3eb-8540aa6c48cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 92s 45ms/step - loss: 0.4414 - accuracy: 0.8396\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.2969 - accuracy: 0.8914\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 94s 50ms/step - loss: 0.2493 - accuracy: 0.9077\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.2171 - accuracy: 0.9188\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 72s 39ms/step - loss: 0.1925 - accuracy: 0.9284\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 78s 42ms/step - loss: 0.1681 - accuracy: 0.9365\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 77s 41ms/step - loss: 0.1470 - accuracy: 0.9446\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 73s 39ms/step - loss: 0.1330 - accuracy: 0.9502\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.1157 - accuracy: 0.9566\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 73s 39ms/step - loss: 0.1032 - accuracy: 0.9613\n",
            "313/313 [==============================] - 5s 13ms/step - loss: 0.3219 - accuracy: 0.9086\n",
            "0.9085999727249146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# Visualización de convoluciones y pooling\n",
        "\n",
        "Este código nos mostrará las convoluciones gráficamente. print(test_labels[;100]) nos muestra las primeras 100 etiquetas en el conjunto de prueba, y se puede ver que las del índice 0, índice 23 e índice 28 tienen el mismo valor (9). Son todos zapatos. Echemos un vistazo al resultado de ejecutar la convolución en cada uno, y comenzarás a ver cómo surgen características comunes entre ellos. Ahora, cuando la DNN está entrenando con esos datos, está trabajando con muchos menos, y tal vez esté encontrando algo en común entre los zapatos en función de esta combinación de convolución/agrupación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-6nX4QsOku6"
      },
      "outputs": [],
      "source": [
        "print(test_labels[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FGsHhv6JvDx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=7\n",
        "THIRD_IMAGE=26\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "EJERCICIOS\n",
        "\n",
        "1. Intenta editar las convoluciones. Cambia el valor 32 a 16 o 64. ¿Qué impacto tendrá esto en la precisión y/o el tiempo de entrenamiento?\n",
        "\n",
        "2. Elimina la convolución final. ¿Qué impacto tendrá esto en la precisión o el tiempo de entrenamiento?\n",
        "\n",
        "3. ¿Qué tal agregar más convoluciones? ¿Qué impacto crees que tendrá esto?\n",
        "\n",
        "4. Elimina todas las circunvoluciones excepto la primera. ¿Qué impacto crees que tendrá esto? \n",
        "\n",
        "5. En la práctica anterior, implementaste un callback para verificar la función de pérdida y cancelar el entrenamiento una vez que alcanza una cierta cantidad. Intenta implementar ese callback también para este modelo:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 1"
      ],
      "metadata": {
        "id": "qPB5ihL9eUSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizaré 5 épocas probando con 32, 16 y 64\n",
        "\n",
        "# Primer modelo (32)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enheh0RZgql5",
        "outputId": "39322e01-2962-4ca7-c720-36413860bdc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 75s 39ms/step - loss: 0.4345 - accuracy: 0.8447\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 0.2941 - accuracy: 0.8925\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.2487 - accuracy: 0.9081\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.2182 - accuracy: 0.9193\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.1916 - accuracy: 0.9289\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.2622 - accuracy: 0.9057\n",
            "0.9057000279426575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Segundo modelo (16)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# Disminuye el tiempo entre épocas en 20 segundos aproximadamente.\n",
        "#\n",
        "# Las métricas loss y accuracy son ligeramente peores \n",
        "# con las imagenes de entrenamiento pero el resultado que se obtiene \n",
        "# de éstas al evaluar el modelo con las imagenes de test es mejor.\n",
        "#\n",
        "# Disminuye el overfitting."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKF9p6UoeWov",
        "outputId": "aa2aef35-3118-4478-e4c1-984c17d8f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.4525 - accuracy: 0.8382\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.3057 - accuracy: 0.8879\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2596 - accuracy: 0.9046\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2290 - accuracy: 0.9160\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2015 - accuracy: 0.9242\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2586 - accuracy: 0.9086\n",
            "0.9085999727249146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tercer modelo - 64\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "#\n",
        "# Dobla el tiempo de ejecución con respecto al primer modelo (32).\n",
        "#\n",
        "# Las métricas loss y accuracy mejoran con ambos conjuntos de \n",
        "# imágenes respecto al primer modelo (32).\n",
        "# \n",
        "# En compración con el segundo modelo mejora la métrica loss pero \n",
        "# no consigue mejorar el accuracy en la evaluación del modelo,\n",
        "# en cambio, en el entrenamiento mejora ambas métricas. \n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD_F80gmewVp",
        "outputId": "d25e856c-79a0-4623-c28b-f4f7a04de976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 118s 62ms/step - loss: 0.4428 - accuracy: 0.8392\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.2937 - accuracy: 0.8912\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 132s 70ms/step - loss: 0.2489 - accuracy: 0.9085\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 117s 62ms/step - loss: 0.2189 - accuracy: 0.9198\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 126s 67ms/step - loss: 0.1898 - accuracy: 0.9292\n",
            "313/313 [==============================] - 7s 20ms/step - loss: 0.2578 - accuracy: 0.9068\n",
            "0.9067999720573425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Como conclusión el segundo modelo (16) es el mejor "
      ],
      "metadata": {
        "id": "kEJklcTauUZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 2"
      ],
      "metadata": {
        "id": "MysCwk_IeC3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# El modelo mejora en todos los aspectos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka4ALpL1eFca",
        "outputId": "66434990-39c3-4473-ff38-319d870144c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 67s 35ms/step - loss: 0.3956 - accuracy: 0.8583\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.2715 - accuracy: 0.9031\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.2247 - accuracy: 0.9177\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.1905 - accuracy: 0.9294\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.1613 - accuracy: 0.9402\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2538 - accuracy: 0.9141\n",
            "0.9140999913215637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3"
      ],
      "metadata": {
        "id": "FBj_HrGQbVAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# Con cuatro convoluciones arroja error. demasiadas capas de reducción o tamaño de imagen pequeño"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "uB1f0JFVOkVo",
        "outputId": "acf51768-cf05-4c3f-d94c-48b37f80fe4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3fffc3b86694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;34m\"One of the dimensions in the output is <= 0 \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;34mf\"due to downsampling in {self.name}. Consider \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_99. Consider increasing the input size. Received input shape [None, 1, 1, 32] which would produce output shape with a zero or negative value in a dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# Este modelo no obtiene buenos resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OG894_puYcV",
        "outputId": "68e36998-85b3-4c22-bd28-978f9eec2cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 56s 29ms/step - loss: 0.5579 - accuracy: 0.7905\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.3525 - accuracy: 0.8710\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 0.3025 - accuracy: 0.8886\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.2733 - accuracy: 0.8988\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.2514 - accuracy: 0.9066\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2807 - accuracy: 0.8999\n",
            "0.8999000191688538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# El tiempo de ejecución disminuye pero el modelo empeora considerablemente el losss y el accuracy.\n",
        "# Dentro de la anterior valoración indicar que cuantas más neuronas peor serían los resultados del modelo,\n",
        "# tras haber probado la convolución intermedia con 16 y 32 neuronas."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n75E_FtIOedo",
        "outputId": "8e6918b0-1f1d-463e-a612-f185d6054b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 62s 32ms/step - loss: 0.6466 - accuracy: 0.7632\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 26ms/step - loss: 0.4436 - accuracy: 0.8367\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.3860 - accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3546 - accuracy: 0.8686\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.3333 - accuracy: 0.8770\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3689 - accuracy: 0.8664\n",
            "0.8664000034332275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "# El tiempo de ejecución disminuye pero el modelo empeora considerablemente el losss y el accuracy.\n",
        "# Dentro de la anterior valoración indicar que cuantas menos neuronas menos tardaría pero peor serían los resultados del modelo,\n",
        "# tras haber probado la convolución intermedia con 16 y 32 neuronas.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhL1Ct3gbXPg",
        "outputId": "5a6742dc-55d4-4339-ccc0-b2703684e6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 71s 37ms/step - loss: 0.6152 - accuracy: 0.7735\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.4143 - accuracy: 0.8482\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.3580 - accuracy: 0.8677\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.3239 - accuracy: 0.8798\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.3001 - accuracy: 0.8882\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3351 - accuracy: 0.8770\n",
            "0.8769999742507935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 4"
      ],
      "metadata": {
        "id": "QhSAI56qdNMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhNvkvkZaCH9",
        "outputId": "a942aa80-79eb-42d9-923f-82ea3c756ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 49s 25ms/step - loss: 0.3955 - accuracy: 0.8592\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2692 - accuracy: 0.9008\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2239 - accuracy: 0.9174\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 65s 34ms/step - loss: 0.1919 - accuracy: 0.9292\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.1647 - accuracy: 0.9382\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2494 - accuracy: 0.9158\n",
            "0.9157999753952026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 5 - loss < 0.3 stop fit()"
      ],
      "metadata": {
        "id": "77up7uYMdT6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('loss')>0.3):\n",
        "      print(\"\\Alcanzado el 30% de error, cancelando entrenamiento\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbJqAl0BinjQ",
        "outputId": "f05d3d26-ac02-415a-d3a9-721fbd825547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.8380\\Alcanzado el 30% de error, cancelando entrenamiento\n",
            "1875/1875 [==============================] - 69s 36ms/step - loss: 0.4437 - accuracy: 0.8380\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.3494 - accuracy: 0.8710\n",
            "0.8709999918937683\n",
            "0.34944334626197815\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}